{"author":{"id":"aed0d72ca16b13a779b1d37a0771f1682d5a59e673ecfbcdd34155713180df84"},"ops":[{"type":1,"timestamp":1702454978,"nonce":"tNtqZB21GFXP6gGo650Rmr15iTk=","metadata":{"github-id":"I_kwDOD3p2KM55i1dG","github-url":"https://github.com/OPEN-NEXT/OKH-LOSH/issues/146","origin":"github"},"title":"RDF IRI(-generation) \u0026 Data Location/Storage","message":"## Current Data Aggregation Process\n\n1. Projects are found on different platforms by different means.\n2. Their meta-data is extracted, either by:\n    1.  just copying the _okh.toml_ file out of their storage/repo\n    2. assembling an _okh.toml_ file by using the hosting platforms API\n3. the TOML data gets converted to RDF\n\n## The problem\n\nThe idea of LinkedData, and very much ours for OKH too, is to support a distributed data system.\nFurthermore, all RDF data - more specifically each subject - is uniquely identifiable by its IRI.\nAn IRI is simply a unicode-version of a URL. \\\nThis pushes two requirements onto us:\n\n1. It is very much recommended - and we should ensure this to be the case -\n    that a **subject is available under its IRI**.\n2. If we generate the RDF on our server (be it centralized or decentralized),\n    and we make it available for the public, we would necessarily have to do it under a domain\n    (which thus becomes an essential part of the RDFs IRI) that we control,\n    and that the original project does not control.\n    This means, the data would not be distributed anymore,\n    and it also means, that each data-collector would host each projects RDF\n    under their URL, using that URL as IRI, which means, we would end up\n    with the same project/data available under different IRIs,\n    which are supposed to be unique identifiers,\n    meaning we would end up with duplicates. \\\n    -\u003e very bad!\n\nWe could choose to do one of two things:\n\n1. use a domain under the control of the original project\n    (e.g. its github pages URL or a perma-URL they registered for this purpose),\n    but actually host the RDF under our own domain, violating the first requirement above, or\n2. host it on our domain, and also using the correct hosting location as its IRI,\n    which satisfies the first requirement above, but violates the second.\n\nIn theory, there is a third option: \\\nEach project generates their RDF by themselfs in a CI, and then hosts it permanently (at least each release version of it plus the latest development one). That though, is very, very unlikely, unstable, difficult to maintain and update, .... and only possible for git-hosted (or other SCM-hosted) projects. \\\n-\u003e not really an option.\n\n---\n\n**_DING, DING, DING, DING, ..._**\n\n:O\nNow, writing the above, I got an idea!\nThere is actually a **fourth option**:\nWe could use a similar approach like W3ID does, to host the data.\nThere is one (or optionally a few -\u003e redundant) git repos, that contain/host all the RDF data.\nMultiple parties that aggregate the data, have push-access to it, and regularly, push to it, in an automated fashion, when crawling/generating the data.\nThis means, both data-gatherers and individual projects could push data.\nThis allows for a somewhat distributed-ish, but at the very least decentralized/federated power over the RDF data,\nand as a *huge* beneficial side-effect, it would allow to efficiently distribute the data-gathering load.","files":null}]}