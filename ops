{"author":{"id":"52c8584678a38419c7d7ec7bae97c4d92731fc757ce0c98565c7ce13fd8aae1c"},"ops":[{"type":3,"timestamp":1617197549,"nonce":"QyD49x8WOTZ1OUNQ/oDKi2n344A=","metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDgxMTA3MTE5OA==","github-url":"https://github.com/OPEN-NEXT/OKH-LOSH/issues/25#issuecomment-811071198"},"message":"@penyuan (I'm cleaning up the issues) the crawler will be published here: https://github.com/OPEN-NEXT/LOSH-krawler (developed by @ahane)\nIt's all python and blocks aim to be as simple as possible. As shown in the [REAMDE](https://github.com/OPEN-NEXT/LOSH#technical-details), the crawler will pull data from selected APIs (which are specified in the crawler), map them onto the ontology published in this repo, create a large RDF, convert it into Wikibases-specific JSON and push it to the selected Wikibase-Instance\n\nTo move data from and to Wikibase; I'd recommend pushing to Wikibase's API directly. @hoijui developed a module that can push RDF ontologies to Wikibase using the old API ([link](https://github.com/hoijui/LOSH-tools) while @addshore is working on a new API which would very much simplify that process ([link](https://github.com/wmde/WikibaseReconcileEdit)).\nOf course there would be also a way to include the dashboard data as RDF and then have it converted by the crawler. However, I do think submitting to Wikibase directly may be easier :D\n\nHope that makes it more or less clear :) For any specific question feel free to re-open this issue or mail me or the developers directly","files":null},{"type":4,"timestamp":1617197549,"nonce":"pYbE90LIbwlG7RevWoZnECkQaek=","metadata":{"github-id":"MDExOkNsb3NlZEV2ZW50NDUzNDc2NzA5NA=="},"status":2}]}