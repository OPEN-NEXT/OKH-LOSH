{"author":{"id":"8f990dab382a59e0f4235d96491a4ee14dc34d3b7e9de61d8875aadd396af416"},"ops":[{"type":3,"timestamp":1600508184,"nonce":"atF2O0r7imkXHHzuJE0UuR2sSDs=","metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY5NTE5MTA5Nw==","github-url":"https://github.com/OPEN-NEXT/OKH-LOSH/issues/24#issuecomment-695191097"},"message":"As you know I've been wrestling with the GitHub API (v3 REST and v4 GraphQL) for the past few months.\n\nSome operations are indeed easy as you described, but others are surprisingly in-efficient. Two examples:\n\n1. I wanted to get every single commit that has been made to a GitHub repository. The GraphQL API forces me query *every branch*, get the list of commits from each, de-duplicate commits that belong to multiple branches, then combine the data. And since the GraphQL API is paginated to only give you 100 results per query, it can take a long time when a repository has thousands of commits across 5-6 branches... -_-''' OTOH, if I use the REST API, I can get the complete list of commits with just a couple of queries, but they returned *tons* of unrelated data and the commits only make up a small fraction of it. I've raised this issue in the GitHub Community forums, and to their credit an actual GitHub employee responded to my concerns but without a better solution or any commitment on improving the API. Oh well.\n\n2. Unsurprisingly, I also want a list of the files that were changed by each commit. This is (directly confirmed by GitHub) *not doable* with the GraphQL API and with the older REST API, doable but also comes with a mountain of unrelated data to weed through.\n\nThat said, I've have no experience with brute-force scraping. My suspicion is that brute-force is computationally expensive and more prone to breakage while using the API is more time-intensive (e.g. waiting for multiple API queries and rate-limiting).\n\nFor WP2.2's dashboard I'm still using the GitHub API. And since I also have to scrape Wikifactory, I hope their API is more friendly. :smile:\n\nI actually kind of like curated lists and semi-regularly visit a few of them. Were you thinking of all those \"awesome lists\" that people maintain on GitLab/GitHub? If those lists have *relatively* consistent layouts/markup, then maybe a crawler for them wouldn't be so bad? My concern would be that those lists usually just link to projects, and once you get to the projects you will still need to mine their actual repositories, many of which are on GitHub et al. and you get back to the API vs brute-force problem?","files":null}]}