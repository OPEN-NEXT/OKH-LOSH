{"author":{"id":"8f990dab382a59e0f4235d96491a4ee14dc34d3b7e9de61d8875aadd396af416"},"ops":[{"type":1,"timestamp":1600508796,"nonce":"dKaYdjbk5VgXvp0HNWtuyv7iqHo=","metadata":{"github-id":"MDU6SXNzdWU3MDQ4NTA3MTg=","github-url":"https://github.com/OPEN-NEXT/OKH-LOSH/issues/25","origin":"github"},"title":"Software architecture of the crawler?","message":"I confess I haven't read *all* the documentation yet, so sorry if I missed something. :sweat_smile:\n\nAs you know I'm trying to mine/crawl the commit histories and issues of GitHub repositories in a learning-by-doing kind of way.\n\nDuring our meeting yesterday, @mkampik made the great point that rather than keeping this part in the dashboard backend, what if we incorporated it into the crawler? That would sound like a neater/tidier implementation rather than essentially maintaining two crawlers (one just for commits and issues tied to the dashboard, and another for everything else). Another advantage is that this might make implementing the data ontology easier.\n\nI suppose this would depend on the architecture and modularity of the crawler. Right now I just have a couple of Python scripts that call the GitHub (and Wikifactory) APIs. Would such incorporation be desirable or feasible? Would the crawler accept a plugin?\n\nI am not super opinionated on this, but curious what others think.","files":null}]}