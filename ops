{"author":{"id":"aed0d72ca16b13a779b1d37a0771f1682d5a59e673ecfbcdd34155713180df84"},"ops":[{"type":5,"timestamp":1702455003,"nonce":"xBwx7dpONXCzbClFh3qcXxvWwCI=","metadata":{"github-id":"LE_lADOD3p2KM55i1dGzwAAAAKdg2Ic"},"added":["idea status"],"removed":[]},{"type":3,"timestamp":1702805033,"nonce":"h2Z9tRhD4r3pWOLQebUwIq2LRnw=","metadata":{"github-id":"IC_kwDOD3p2KM5uz1_q","github-url":"https://github.com/OPEN-NEXT/OKH-LOSH/issues/146#issuecomment-1859084266"},"message":"**_DING, DING, DING, DING, ..._**\n\n:O\nNow, writing the above, I got an idea!\nThere is actually a **fourth option**:\nWe could use a similar approach like W3ID does, to host the data.\nThere is one (or optionally a few -\u003e redundant) git repos, that contain/host all the RDF data.\nMultiple parties that aggregate the data, have push-access to it, and regularly, push to it, in an automated fashion, when crawling/generating the data.\nThis means, both data-gatherers and individual projects could push data.\nThis allows for a somewhat distributed-ish, but at the very least decentralized/federated power over the RDF data,\nand as a *huge* beneficial side-effect, it would allow to efficiently distribute the data-gathering load.","files":null}]}