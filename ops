{"author":{"id":"aed0d72ca16b13a779b1d37a0771f1682d5a59e673ecfbcdd34155713180df84"},"ops":[{"type":3,"timestamp":1613680776,"nonce":"Q4ez3U5c4EqAohKxaQt0yy33jj0=","metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc4MTYyMDYyNg==","github-url":"https://github.com/OPEN-NEXT/OKH-LOSH/issues/74#issuecomment-781620626"},"message":"+1 on allowing the (s)BOM to reside in an other place (generated).\n\nMy first impulse is, to keep the current approach of sBOM.csv (in a flexible location) for the demonstrator,\nbut keep that part of the crawler modular, so it would be easy to support multiple formats.\nThe key idea behind this, is that the real BOM is the resulting RDF, and how we get to it, is secondary.\n\nI imagine a scheme, where wikifactory for example, have their BOM data in a totally different format (e.g. an SQL database), and are able to offer it through an API specific to them, including a conversion script to generate RDF from it, that the crawler could call for each project.\n\nI also like the idea of different levels of completeness of the BOM (only MOSHes and POSHes, or also BUY and STD parts), which will lead to different completeness scores.\nI woudl liek to have all that data for at least some of our example projects, though.","files":null},{"type":6,"timestamp":1613680801,"nonce":"bFkAyPELiW3rY29TsOWnHFgDrVU=","metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDM2MTQyNDg0"},"target":"5c73383b1e5dc9811b41fed79199ade90d01161fe8a0cb5b2f91d19f3d961f47","message":"+1 on allowing the (s)BOM to reside in an other place (generated).\n\nMy first impulse is, to keep the current approach of sBOM.csv (in a flexible location) for the demonstrator,\nbut keep that part of the crawler modular, so it would be easy to support multiple formats.\nThe key idea behind this, is that the real BOM is the resulting RDF, and how we get to it, is secondary.\n\nI imagine a scheme, where wikifactory for example, have their BOM data in a totally different format (e.g. an SQL database), and are able to offer it through an API specific to them, including a conversion script to generate RDF from it, that the crawler could call for each project.\n\nI also like the idea of different levels of completeness of the BOM (only MOSHes and POSHes, or also BUY and STD parts), which will lead to different completeness scores.\nI would like to have all that data for at least some of our example projects, though.","files":null}]}